{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c916395-b1e0-4167-abed-5bd0e9710448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from torch import optim\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "87bb7547-448b-4028-960f-64649de37a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        Date       Open       High        Low      Close  \\\n",
      "0  2000-01-03 00:00:00-05:00  25.215814  25.330655  24.690826  24.772856   \n",
      "1  2000-01-04 00:00:00-05:00  24.379101  24.887683  23.788490  23.788490   \n",
      "2  2000-01-05 00:00:00-05:00  23.919738  25.265018  23.919738  24.477537   \n",
      "\n",
      "      Volume Brand_Name Ticker   Industry_Tag Country  Dividends  \\\n",
      "0  2173400.0         3m    MMM  manufacturing     usa        0.0   \n",
      "1  2713800.0         3m    MMM  manufacturing     usa        0.0   \n",
      "2  3699400.0         3m    MMM  manufacturing     usa        0.0   \n",
      "\n",
      "   Stock Splits  Capital Gains   Date_only    Margin  Ticker_id  \\\n",
      "0           0.0            NaN  2000-01-03  0.442958          0   \n",
      "1           0.0            NaN  2000-01-04  0.590611          0   \n",
      "2           0.0            NaN  2000-01-05 -0.557799          0   \n",
      "\n",
      "   Normalized_Close  \n",
      "0          0.020708  \n",
      "1          0.015483  \n",
      "2          0.019141  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Reading the file that contains stock continious sequences\n",
    "stocks_df = pd.read_csv('stock_dataframe.csv')\n",
    "print(stocks_df.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e222c161-5858-4176-ba53-f3db61aa5b8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating ticker:integer_id dictionary\n",
    "ticker2idx = {ticker: i for i, ticker in enumerate(stocks_df[\"Ticker\"].unique())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "004adaa4-a7e3-4671-96f2-0e04aa93a426",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#  Function to create  the  training-test dataset from the dataframe that contains  time-series sequences of stock prices \n",
    "def create_sequences(df, seq_length=4, features=[\"Open\", \"Close\"], split_ratio=0.8):\n",
    "    X_train, y_train, ids_train = [], [], []\n",
    "    X_test, y_test, ids_test = [], [], []\n",
    "    ticker2idx = {ticker: i for i, ticker in enumerate(df[\"Ticker\"].unique())}\n",
    "    group_length = []\n",
    "    for ticker, group in df.groupby(\"Ticker\"):\n",
    "        group = group.sort_values(\"Date\").reset_index(drop=True)\n",
    "        ticker_id = ticker2idx[ticker]\n",
    "\n",
    "        ticker_sequences = []\n",
    "        ticker_labels = []\n",
    "        \n",
    "        group_length.append(len(group))\n",
    "        for i in range(len(group) - seq_length):\n",
    "            seq = group.iloc[i:i+seq_length][features].values\n",
    "            label = group.iloc[i+seq_length][\"Close\"]\n",
    "\n",
    "            ticker_sequences.append(seq)\n",
    "            ticker_labels.append(label)\n",
    "\n",
    "        split_idx = int(split_ratio * len(ticker_sequences)) #0.8 ,0.2 \n",
    " \n",
    "        X_train.extend(ticker_sequences[:split_idx])\n",
    "        y_train.extend(ticker_labels[:split_idx])\n",
    "        ids_train.extend([ticker_id] * split_idx)\n",
    "\n",
    "        X_test.extend(ticker_sequences[split_idx:])\n",
    "        y_test.extend(ticker_labels[split_idx:])\n",
    "        ids_test.extend([ticker_id] * (len(ticker_sequences) - split_idx))\n",
    "\n",
    "    return (\n",
    "        np.array(X_train), np.array(y_train), np.array(ids_train),\n",
    "        np.array(X_test), np.array(y_test), np.array(ids_test),group_length\n",
    "    )\n",
    "# Calling create_sequences() to formate  the time sequences of stock prices for training-test dataset\n",
    "X_train, y_train, ids_train, X_test, y_test, ids_test,group_length = create_sequences(stocks_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "69736743-8a52-43e1-a6c6-72de3762c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Groups with less than 32 samples-sequences(160 days) : \n",
      " 0\n",
      "errors : \n",
      " 3\n"
     ]
    }
   ],
   "source": [
    "# Testing if a group of records have a batch with less  than 32 samples \n",
    "counter_of_groups = 0\n",
    "counter_of_error =0 \n",
    "for group_num in group_length :\n",
    "    \n",
    "    if group_num < 160:\n",
    "       counter_of_groups =counter_of_groups +1\n",
    "    if  group_num*0.2<160:\n",
    "          counter_of_error = counter_of_error+1\n",
    "       #if len(group_num)*0.832: \n",
    "print('Num of Groups with less than 32 samples-sequences(160 days) : \\n',counter_of_groups)\n",
    "print('errors : \\n',counter_of_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee52c766-8685-4027-8917-caa937e300ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Dataset Class\n",
    "class StockDataset(Dataset):\n",
    "    def __init__(self, X, y,tickers_ids): #ticker_ids\n",
    "        #print('This is array X : \\n',X)\n",
    "        self.X = torch.tensor(X, dtype=torch.float32) #Check X contents before and after conversion\n",
    "        #print('This is Tensor X : \\n',X)\n",
    "        self.y = torch.tensor(y, dtype=torch.float32)\n",
    "        self.ticker_ids = torch.tensor(tickers_ids, dtype=torch.long)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)#,self.y,self.tickers_ids)\n",
    "       # return len(self.y)  # print(len(dataset))         # calls __len__ → returns number of samples\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.y[idx] ,self.ticker_ids[idx] #    self.y[idx].unsqueeze(0) adding   # print(dataset[0])  calls __getitem__(0) → returns (x, y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "defd7fda-62e5-4973-991e-6f20748800d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StockPredictor(nn.Module) :  \n",
    "   def __init__(self,embedding_dim,num_tickers,hidden_size,output_size,num_layers,num_features): # Constructor method\n",
    "    super(StockPredictor, self).__init__()  #Inheriting \n",
    "    self.embedding = nn.Embedding(num_tickers,embedding_dim) #num_tickers -> Size of tickers:id dictionary  (number of tickers for embedding)\n",
    "      \n",
    "    self.lstm = nn.LSTM(input_size=num_features+embedding_dim, ##input_size = num_features \n",
    "                        hidden_size=hidden_size,\n",
    "                        num_layers=num_layers,\n",
    "                        batch_first=True)  # if batch_ first true : (batch, seq, feature) instead of (seq, batch, feature).\n",
    "    self.fc = nn.Linear( hidden_size,output_size) \n",
    "\n",
    "   def forward(self,x,ticker_ids): \n",
    "    ''' \n",
    "     Notes:\n",
    "     # x: (batch, seq_len, input_size-num_features)\n",
    "     # output: (batch,seq_len,hidden_size)  each row contains hidden state vecotr for each time step  of  last layer\n",
    "     # output[:,-1,:] picking the last hidden state vector (last row)  for each sample #hidden state vector size is equal to the number of hidden_size\n",
    "     # ticker_ids: (batch,)\n",
    "     \n",
    "     # h_n: [num_layers,seq_len,hidden_size]\tFinal hidden states at last time step of each layer\n",
    "     #c_n:  [num_layers,seq_len,hidden_size] Final cell states at last time step of each layer\n",
    "     #output[t] = hidden states at time step t → shape [seq_length,hidden_size]\n",
    "     # h_n[layer] = hidden state at last time step for that layer -> h_n[0] = output from layer 0's final time step \n",
    "          stock_embeddings = self.embedding(ticker_ids)   # (batch, embedding_dim)  \n",
    "    \n",
    "     stock_embeddings.shape = (32, 16)\n",
    "     #stock_embeddings.unsqueeze(1) = (32, 1, 16)\n",
    "     #stock_embeddings = stock_embeddings.repeat(1, 4, 1) => (32, 1, 16) → (32, 4, 16) each 'fake'time step has its own word embedding and  repeat copies the embedding across all seq_length time steps\n",
    "     '''\n",
    "    stock_embeddings = self.embedding(ticker_ids)   # (batch, embedding_dim)  \n",
    "    stock_embeddings = stock_embeddings.unsqueeze(1).repeat(1, x.size(1), 1) # (batch, seq_len, embedding_dim) #size(1) picks the second dimension\n",
    "    # Concatenate embeddings along the feature dimension\n",
    "    x = torch.cat([x,stock_embeddings],dim=-1)  #  (batch, seq_len, input_size + embedding_dim)     #other way ->: x = torch.cat((x,  stock_embeddings), dim=2)\n",
    "    output,(h_n, c_n) = self.lstm(x)  # batch_size(N), seq_len,  # other notation for output-> output, (h_n, c_n) =  self.lstm(x) Hidden states from all time steps of the last LSTM layer only.\n",
    "   #print('Hidden state h_n shape : ',h_n.shape)  #(Final hidden state of each layer, for the last time step) (num_layers * num_directions, batch_size, hidden_size)\n",
    "   # print('\\n Cell state c_n :',c_n.size()) #size() or .shape\n",
    "    last_time_step = output[:, -1, :]  # Picking the last hidden state for each sample-sequence  #last hidden state for each sample\n",
    "    stock_predictions  = self.fc(last_time_step)  # Using last hidden states through linear fcFinal prediction:   ->   (batch,1)\n",
    "    return stock_predictions\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "59069cf8-9612-4906-bbde-f012a713f7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating test-train Dataset\n",
    "train_dataset = StockDataset(X_train, y_train,ids_train)\n",
    "test_dataset = StockDataset(X_test, y_test, ids_test)\n",
    "# Passing the dataset to test-train Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True,drop_last=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16, shuffle=False,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50300d60-111a-4263-bf2a-2c574d382bf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training Samples: 201276\n",
      "Number of test Samples 50380\n"
     ]
    }
   ],
   "source": [
    "\n",
    "batch = next(iter(train_loader))\n",
    "# Unpack the batch\n",
    "tensor_seq, labels, ticker_ids = batch\n",
    "print(\"Number of training Samples:\",len(train_dataset))\n",
    "print(\"Number of test Samples\",len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5282c0f4-e43c-4994-bb5d-aa973d21a6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Model Parameters \n",
    "embedding_dim = 8\n",
    "num_tickers = len(ticker2idx)\n",
    "hidden_size = 128\n",
    "output_size=1\n",
    "num_layers=3 \n",
    "batch_size=32\n",
    "num_features = int(tensor_seq.size(-1)) #num_features torch dimension\n",
    "lr=0.001  # Learning rate\n",
    "# Creating model\n",
    "Stock_predictor =StockPredictor(embedding_dim=embedding_dim,num_tickers= num_tickers, hidden_size=hidden_size,output_size=output_size,num_layers=num_layers,num_features=num_features)\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(Stock_predictor.parameters(),lr=lr)\n",
    "# Criterion-Loss_Function\n",
    "loss_function= nn.MSELoss() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "571e5ab9-0469-4379-9b51-774648a245e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def training_function (model,train_loader,loss_function,optimizer,num_epochs): #+ device='cpu' #\n",
    "\n",
    " # Hyperparameters\n",
    " num_epochs=num_epochs\n",
    "\n",
    " # Set model for training\n",
    " model.train()\n",
    "\n",
    " # Training loop    \n",
    " for epoch in range (num_epochs):\n",
    "     running_loss=0\n",
    "     for idx,(sequence,labels,ticker_ids) in enumerate(train_loader,start=1):\n",
    "      labels = labels.view(-1, 1)  # ensures shape [batch_size, 1]\n",
    "     #forward() prediction\n",
    "      predictions = Stock_predictor(sequence,ticker_ids)\n",
    "      # Loss function\n",
    "      loss=loss_function(predictions,labels) # This is a tensor (scalar tensor)\n",
    "      # Zero Grad\n",
    "      optimizer.zero_grad()\n",
    "      #Back propagation\n",
    "      loss.backward()\n",
    "      \n",
    "      # Optimizer step\n",
    "      optimizer.step()\n",
    "     # if idx%32==0 :\n",
    "      #print(accuracy_fn(predictions,labels)) #we dont use accuracy in regression because the numbers are close estimations hard to have 1-1 match\n",
    "      # Track the running loss\n",
    "      running_loss += loss.item() #loss_value = loss.item()  # Converts single-element tensor -> float\n",
    "        \n",
    "\n",
    "     # Print loss for every epoch\n",
    "     print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6100bd45-41ac-476a-ac75-07fdcc38c08b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50, Loss: 2164.2261707378575\n",
      "Epoch 2/50, Loss: 606.1973296576235\n",
      "Epoch 3/50, Loss: 337.893788085473\n",
      "Epoch 4/50, Loss: 200.57270093698875\n",
      "Epoch 5/50, Loss: 173.92933164771523\n",
      "Epoch 6/50, Loss: 230.68768862849922\n",
      "Epoch 7/50, Loss: 236.21091439942094\n",
      "Epoch 8/50, Loss: 174.80154826461387\n",
      "Epoch 9/50, Loss: 141.0195274433042\n",
      "Epoch 10/50, Loss: 124.29000898147922\n",
      "Epoch 11/50, Loss: 166.9941652658459\n",
      "Epoch 12/50, Loss: 124.91790830391942\n",
      "Epoch 13/50, Loss: 118.21204648635681\n",
      "Epoch 14/50, Loss: 146.8734488595952\n",
      "Epoch 15/50, Loss: 131.55231296872321\n",
      "Epoch 16/50, Loss: 135.0920653179096\n",
      "Epoch 17/50, Loss: 140.91049451731047\n",
      "Epoch 18/50, Loss: 141.72822759839863\n",
      "Epoch 19/50, Loss: 231.7305499640794\n",
      "Epoch 20/50, Loss: 183.3345580881873\n",
      "Epoch 21/50, Loss: 145.35376166721363\n",
      "Epoch 22/50, Loss: 197.96053347462575\n",
      "Epoch 23/50, Loss: 161.06431647850383\n",
      "Epoch 24/50, Loss: 144.0351850611357\n",
      "Epoch 25/50, Loss: 178.59761438547787\n",
      "Epoch 26/50, Loss: 193.6725819332362\n",
      "Epoch 27/50, Loss: 198.67618194984212\n",
      "Epoch 28/50, Loss: 157.46246906164143\n",
      "Epoch 29/50, Loss: 205.28473400909942\n",
      "Epoch 30/50, Loss: 193.1547004223429\n",
      "Epoch 31/50, Loss: 186.39042515703946\n",
      "Epoch 32/50, Loss: 178.00569724175344\n",
      "Epoch 33/50, Loss: 146.77684371390419\n",
      "Epoch 34/50, Loss: 110.13365920525388\n",
      "Epoch 35/50, Loss: 128.65917523578935\n",
      "Epoch 36/50, Loss: 102.74395590824398\n",
      "Epoch 37/50, Loss: 101.60221362563902\n",
      "Epoch 38/50, Loss: 108.24245947260823\n",
      "Epoch 39/50, Loss: 96.18680810341208\n",
      "Epoch 40/50, Loss: 117.71211805463425\n",
      "Epoch 41/50, Loss: 104.74933278306165\n",
      "Epoch 42/50, Loss: 137.17542390433172\n",
      "Epoch 43/50, Loss: 174.01241937115265\n",
      "Epoch 44/50, Loss: 97.29109171121758\n",
      "Epoch 45/50, Loss: 122.35531504422255\n",
      "Epoch 46/50, Loss: 111.65488712273041\n",
      "Epoch 47/50, Loss: 68.29018697183118\n",
      "Epoch 48/50, Loss: 55.96247708904315\n",
      "Epoch 49/50, Loss: 58.667807806287385\n",
      "Epoch 50/50, Loss: 76.93635760079954\n"
     ]
    }
   ],
   "source": [
    "training_function(model=Stock_predictor,train_loader=train_loader,loss_function=loss_function,optimizer=optimizer,num_epochs=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f77064-63da-42cb-93eb-979fb67ed27c",
   "metadata": {},
   "source": [
    "## Norrmalization ,feature optimization and other strategies to optimize loss   are needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c09d34-c3dc-4fe5-803b-8635cc0a5c36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(model,test_loader,loss_function): #, device='cpu'\n",
    "   model.eval()\n",
    "   total_loss = 0.0\n",
    "   all_predictions = []\n",
    "   all_labels = []\n",
    "   total_samples = 0.0\n",
    "   with torch.no_grad():  # Turn off gradient tracking\n",
    "          for idx,(sequence,labels,ticker_ids) in enumerate(test_loader,start=1):\n",
    "            #inputs = inputs.to(device)\n",
    "            #labels = labels.to(device)\n",
    "\n",
    "            stock_predictions = model(sequence,ticker_ids)  # Forward pass\n",
    "            labels = labels.unsqueeze(1)\n",
    "            loss = loss_function(stock_predictions, labels)\n",
    "            #labels = labels.unsqueeze(1)\n",
    "            total_loss += loss.item()\n",
    "            all_predictions.append(stock_predictions)  # all_predictions.append(outputs.cpu())\n",
    "            all_labels.append(labels)\n",
    "            total_samples += batch_size\n",
    "            if idx%32==0 : \n",
    "             predictions = torch.cat(all_predictions).squeeze()\n",
    "             labels = torch.cat(all_labels).squeeze()\n",
    "             print(\"f:predictions:\\n {} \\n  vs \\n labels: {}\".format( predictions, labels) )\n",
    "         # number_of_batches = len(test_loader)/32\n",
    "          avg_loss = total_loss /  total_samples          \n",
    "          print(f\"Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "62f32d7f-8070-4741-9e4e-0742e0003f73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846, 0.0846, 0.0847, 0.0846, 0.0846, 0.0846, 0.0845,\n",
      "        0.0846, 0.0846, 0.0845, 0.0845, 0.0845, 0.0845, 0.0845, 0.0844, 0.0844,\n",
      "        0.0844, 0.0844, 0.0844, 0.0844, 0.0843, 0.0842, 0.0843, 0.0842, 0.0842,\n",
      "        0.0841, 0.0841, 0.0842, 0.0842, 0.0841, 0.0840, 0.0840, 0.0840, 0.0840,\n",
      "        0.0840, 0.0840, 0.0839, 0.0839, 0.0839, 0.0839, 0.0838, 0.0838, 0.0838,\n",
      "        0.0838, 0.0839, 0.0839, 0.0840, 0.0839, 0.0838, 0.0838, 0.0838, 0.0839,\n",
      "        0.0839, 0.0838, 0.0837, 0.0837, 0.0836, 0.0836, 0.0835, 0.0835, 0.0835,\n",
      "        0.0833, 0.0833, 0.0832, 0.0831, 0.0831, 0.0831, 0.0831, 0.0832, 0.0832,\n",
      "        0.0831, 0.0831, 0.0830, 0.0831, 0.0831, 0.0831, 0.0830, 0.0831, 0.0831,\n",
      "        0.0831, 0.0831, 0.0830, 0.0831, 0.0831, 0.0832, 0.0834, 0.0836, 0.0841,\n",
      "        0.0840, 0.0839, 0.0836, 0.0834, 0.0837, 0.0837, 0.0839, 0.0839, 0.0839,\n",
      "        0.0840, 0.0842, 0.0844, 0.0843, 0.0843, 0.0842, 0.0841, 0.0842, 0.0844,\n",
      "        0.0844, 0.0842, 0.0843, 0.0840, 0.0841, 0.0841, 0.0841, 0.0842, 0.0840,\n",
      "        0.0838, 0.0836, 0.0835, 0.0835, 0.0836, 0.0836, 0.0837, 0.0838, 0.0837,\n",
      "        0.0836, 0.0835, 0.0835, 0.0836, 0.0836, 0.0835, 0.0834, 0.0833, 0.0833,\n",
      "        0.0833, 0.0832, 0.0832, 0.0832, 0.0833, 0.0833, 0.0832, 0.0831, 0.0831,\n",
      "        0.0831, 0.0831, 0.0831, 0.0830, 0.0830, 0.0830, 0.0831, 0.0831, 0.0829,\n",
      "        0.0827, 0.0827, 0.0827, 0.0827, 0.0827, 0.0825, 0.0826, 0.0826, 0.0825,\n",
      "        0.0825, 0.0824, 0.0824, 0.0824, 0.0823, 0.0823, 0.0823, 0.0822, 0.0822,\n",
      "        0.0822, 0.0821, 0.0822, 0.0822, 0.0821, 0.0821, 0.0821, 0.0822, 0.0823,\n",
      "        0.0823, 0.0824, 0.0824, 0.0824, 0.0823, 0.0820, 0.0819, 0.0819, 0.0817,\n",
      "        0.0816, 0.0816, 0.0816, 0.0816, 0.0816, 0.0815, 0.0815, 0.0815, 0.0815,\n",
      "        0.0818, 0.0815, 0.0814, 0.0813, 0.0810, 0.0810, 0.0811, 0.0811, 0.0809,\n",
      "        0.0808, 0.0810, 0.0811, 0.0812, 0.0813, 0.0814, 0.0814, 0.0818, 0.0819,\n",
      "        0.0817, 0.0819, 0.0819, 0.0818, 0.0818, 0.0817, 0.0815, 0.0815, 0.0816,\n",
      "        0.0815, 0.0815, 0.0814, 0.0814, 0.0816, 0.0814, 0.0814, 0.0814, 0.0812,\n",
      "        0.0812, 0.0813, 0.0813, 0.0813, 0.0814, 0.0814, 0.0815, 0.0814, 0.0816,\n",
      "        0.0816, 0.0815, 0.0817, 0.0816, 0.0816, 0.0815, 0.0813, 0.0813, 0.0814,\n",
      "        0.0814, 0.0814, 0.0814, 0.0813, 0.0813, 0.0814, 0.0813, 0.0814, 0.0815,\n",
      "        0.0814, 0.0814, 0.0813, 0.0813, 0.0812, 0.0811, 0.0813, 0.0813, 0.0812,\n",
      "        0.0814, 0.0813, 0.0812, 0.0811, 0.0808, 0.0809, 0.0810, 0.0811, 0.0811,\n",
      "        0.0811, 0.0811, 0.0811, 0.0811, 0.0810, 0.0809, 0.0807, 0.0806, 0.0806,\n",
      "        0.0806, 0.0807, 0.0808, 0.0807, 0.0809, 0.0808, 0.0809, 0.0809, 0.0808,\n",
      "        0.0808, 0.0809, 0.0809, 0.0811, 0.0812, 0.0811, 0.0811, 0.0813, 0.0811,\n",
      "        0.0811, 0.0811, 0.0811, 0.0811, 0.0812, 0.0812, 0.0812, 0.0813, 0.0813,\n",
      "        0.0813, 0.0813, 0.0812, 0.0812, 0.0814, 0.0813, 0.0812, 0.0813, 0.0813,\n",
      "        0.0813, 0.0813, 0.0813, 0.0813, 0.0813, 0.0812, 0.0811, 0.0810, 0.0809,\n",
      "        0.0809, 0.0809, 0.0809, 0.0809, 0.0809, 0.0810, 0.0810, 0.0809, 0.0809,\n",
      "        0.0808, 0.0808, 0.0809, 0.0808, 0.0809, 0.0810, 0.0810, 0.0810, 0.0811,\n",
      "        0.0811, 0.0812, 0.0813, 0.0811, 0.0811, 0.0812, 0.0812, 0.0811, 0.0812,\n",
      "        0.0811, 0.0810, 0.0811, 0.0810, 0.0810, 0.0811, 0.0811, 0.0811, 0.0811,\n",
      "        0.0812, 0.0811, 0.0811, 0.0811, 0.0811, 0.0811, 0.0812, 0.0811, 0.0810,\n",
      "        0.0809, 0.0809, 0.0810, 0.0810, 0.0810, 0.0810, 0.0808, 0.0808, 0.0808,\n",
      "        0.0806, 0.0806, 0.0806, 0.0807, 0.0807, 0.0808, 0.0808, 0.0807, 0.0806,\n",
      "        0.0806, 0.0807, 0.0807, 0.0807, 0.0808, 0.0807, 0.0807, 0.0807, 0.0806,\n",
      "        0.0806, 0.0806, 0.0807, 0.0807, 0.0808, 0.0808, 0.0806, 0.0807, 0.0807,\n",
      "        0.0807, 0.0807, 0.0806, 0.0806, 0.0806, 0.0808, 0.0807, 0.0807, 0.0806,\n",
      "        0.0805, 0.0805, 0.0805, 0.0805, 0.0806, 0.0805, 0.0806, 0.0806, 0.0806,\n",
      "        0.0807, 0.0807, 0.0807, 0.0807, 0.0807, 0.0806, 0.0807, 0.0806, 0.0807,\n",
      "        0.0809, 0.0808, 0.0808, 0.0808, 0.0807, 0.0807, 0.0808, 0.0808, 0.0809,\n",
      "        0.0809, 0.0808, 0.0808, 0.0807, 0.0806, 0.0806, 0.0806, 0.0807, 0.0808,\n",
      "        0.0808, 0.0808, 0.0808, 0.0806, 0.0806, 0.0805, 0.0805, 0.0805, 0.0805,\n",
      "        0.0806, 0.0806, 0.0806, 0.0807, 0.0808, 0.0809, 0.0808, 0.0809, 0.0806,\n",
      "        0.0808, 0.0807, 0.0805, 0.0806, 0.0805, 0.0805, 0.0806, 0.0803, 0.0802,\n",
      "        0.0804, 0.0801, 0.0803, 0.0804, 0.0802, 0.0803, 0.0803, 0.0802, 0.0804,\n",
      "        0.0803, 0.0801, 0.0802, 0.0801, 0.0803, 0.0805, 0.0804, 0.0803, 0.0803,\n",
      "        0.0804, 0.0805, 0.0804, 0.0803, 0.0804, 0.0806, 0.0806, 0.0806]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  54.4581,  54.6088,  53.2399,  53.6921,\n",
      "         55.1972,  55.2094,  54.5626,  55.2021,  55.9461,  57.4342,  57.3515,\n",
      "         57.2178,  56.9868,  57.2081,  57.4828,  58.4797,  58.3460,  59.1289,\n",
      "         59.2262,  59.9557,  60.5562,  59.1557,  59.1484,  60.4857,  62.2023,\n",
      "         62.6108,  62.5209,  62.5476,  63.2695,  63.4427,  63.9451,  63.8865,\n",
      "         64.4986,  64.0523,  64.8133,  65.1400,  64.9425,  64.1865,  63.8987,\n",
      "         63.8426,  64.4230,  63.2744,  63.8329,  64.7694,  66.0204,  65.0962,\n",
      "         65.4766,  66.0351,  66.2034,  67.1033,  68.2519,  68.3861,  68.2227,\n",
      "         68.2910,  68.1495,  73.1149,  72.7710,  73.9416,  75.5122,  75.6829,\n",
      "         77.2999,  76.2560,  75.9293,  76.8804,  77.7315,  75.3464,  77.4779,\n",
      "         79.0997,  78.9851,  75.4830,  75.2757,  77.7608,  78.3949,  79.3118,\n",
      "         78.2338,  78.6054,  78.1311,  79.9866,  79.4170,  79.4365,  72.8924,\n",
      "         70.4234,  71.5405,  66.8641,  66.8250,  73.0464,  70.7265,  74.0071,\n",
      "         71.6065,  70.6556,  65.0673,  69.7536,  67.3310,  60.6817,  67.9519,\n",
      "         59.2101,  61.8136,  60.3004,  59.8384,  56.0395,  54.8490,  60.3517,\n",
      "         60.0193,  63.1777,  60.5619,  62.2903,  62.1631,  58.8923,  59.8750,\n",
      "         59.0145,  66.7980,  70.1716,  69.5311,  70.0836,  69.1326,  67.6977,\n",
      "         65.6051,  67.4948,  67.2332,  69.1742,  69.2231,  68.1010,  70.3378,\n",
      "         71.8217,  70.6654,  71.6652,  72.7408,  73.4913,  74.2516,  76.0189,\n",
      "         77.2151,  76.3327,  75.4110,  75.8743,  75.4257,  77.2028,  76.7567,\n",
      "         78.2495,  77.6661,  78.1661,  78.8917,  79.2569,  79.6932,  79.0069,\n",
      "         81.2571,  81.7375,  84.3186,  86.4879,  82.3356,  83.0465,  84.0735,\n",
      "         86.3016,  86.1815,  86.2158,  85.7232,  87.9660,  89.8436,  88.2577,\n",
      "         89.4294,  86.6816,  91.6379,  91.3536,  93.4812,  93.8832,  94.0474,\n",
      "         93.6136,  95.1627,  95.8172,  94.6382,  94.4470,  96.4373,  95.1063,\n",
      "         95.3735,  91.0325,  90.8069,  92.9591,  91.4320,  93.1846,  94.3121,\n",
      "        104.1856, 106.8108, 107.5241, 107.9138, 111.6788, 109.1397, 110.7261,\n",
      "        107.4331, 111.0036, 112.9680, 112.8674, 112.5727, 113.5107, 113.6532,\n",
      "        116.1751, 122.1619, 123.6230, 122.6088, 124.2761, 122.7905, 122.5916,\n",
      "        126.7490, 131.7977, 129.0671, 118.7338, 118.8124, 113.3118, 113.4886,\n",
      "        110.1392, 108.3810, 104.9431, 108.1256, 109.8249, 105.2181, 106.2986,\n",
      "        110.2865, 112.9189, 112.0644, 113.7539, 114.7165, 111.0134, 114.4316,\n",
      "        111.1509, 113.0368, 112.9288, 114.8933, 122.1913, 118.9499, 119.0383,\n",
      "        118.5668, 116.9069, 113.9208, 115.4237, 114.7950, 113.6949, 112.9975,\n",
      "        113.0074, 114.5298, 109.2257, 113.2725, 106.9272, 106.8388, 108.4792,\n",
      "        112.9091, 116.9167, 116.7839, 114.4519, 114.1075, 117.5710, 117.2955,\n",
      "        117.3447, 118.3680, 117.4726, 116.1345, 116.7346, 115.4555, 117.1381,\n",
      "        120.7491, 121.1033, 120.9656, 120.2867, 121.7626, 122.3825, 119.8242,\n",
      "        121.2608, 120.4441, 119.8242, 125.8263, 125.7574, 126.6331, 124.6259,\n",
      "        127.3317, 128.9060, 124.5668, 128.8174, 129.9293, 126.9086, 126.7315,\n",
      "        128.7879, 126.8397, 125.0981, 140.6247, 140.8609, 139.7785, 134.8884,\n",
      "        129.8407, 131.9857, 132.8221, 131.7889, 135.1836, 134.7647, 134.9125,\n",
      "        134.0257, 133.4147, 133.1585, 133.3950, 124.1617, 124.0238, 123.5212,\n",
      "        119.2248, 119.4909, 125.9256, 123.2946, 120.2792, 118.3774, 119.6485,\n",
      "        114.6624, 119.3233, 118.2295, 120.1806, 119.2642, 122.1811, 123.7380,\n",
      "        122.9398, 118.7715, 118.2394, 121.5898, 120.7522, 118.3379, 118.8306,\n",
      "        119.4416, 124.0632, 124.3687, 126.0340, 128.4581, 131.0596, 129.3253,\n",
      "        132.4687, 130.1037, 132.5377, 132.2027, 132.8727, 131.1680, 131.5523,\n",
      "        130.0151, 132.3603, 132.7545, 132.4293, 131.6311, 131.5326, 129.5421,\n",
      "        130.6063, 125.9847, 126.2311, 127.8471, 128.5282, 125.2116, 124.2838,\n",
      "        121.1843, 123.3559, 125.8039, 124.6391, 123.2375, 123.0795, 125.6657,\n",
      "        123.8100, 125.4584, 125.2610, 125.2116, 123.6619, 123.0006, 124.2739,\n",
      "        125.1031, 125.4880, 124.4812, 125.7052, 128.7948, 127.9656, 128.4690,\n",
      "        130.0878, 128.7750, 130.5912, 132.2496, 131.9732, 131.6869, 131.3908,\n",
      "        133.0392, 134.5692, 135.1911, 135.4971, 138.1523, 142.6337, 143.7590,\n",
      "        147.2236, 146.5623, 144.4993, 140.6101, 144.2624, 143.5220, 144.9040,\n",
      "        146.6412, 147.0657, 144.8744, 143.1075, 143.7590, 143.9761, 143.6405,\n",
      "        145.4568, 145.0520, 145.1606, 144.4686, 144.4192, 143.9348, 144.1918,\n",
      "        147.1872, 147.3948, 149.3917, 148.4723, 144.6861, 145.0222, 146.4952,\n",
      "        147.9978, 147.9088, 146.6632, 145.8526, 146.9005, 151.3688, 150.0936,\n",
      "        150.7657, 151.8927, 152.5353, 147.8396, 146.4259, 147.3255, 147.0883,\n",
      "        144.3896, 141.3052, 141.7896, 144.1819, 145.1507, 145.2397, 143.7074,\n",
      "        140.2870, 141.1965, 139.8817, 141.0185, 137.5487, 139.4962, 140.3760,\n",
      "        141.6512, 141.2657, 141.1767, 139.8916, 139.2984, 142.1159, 143.1835,\n",
      "        144.8739, 147.0586, 147.5529, 147.7704, 146.9895, 146.9400, 147.6123,\n",
      "        147.1476, 150.8251, 148.0868, 147.2564, 148.3042, 149.7574, 149.2335,\n",
      "        149.7681, 148.9365, 149.3028, 146.4417, 146.3922, 148.4910, 148.5009,\n",
      "        149.4909, 151.9560, 156.2922, 158.9454, 158.6385, 163.6480, 163.1233,\n",
      "        162.1234, 160.2225, 163.6678, 169.4692, 173.3302, 172.8154, 177.6566,\n",
      "        173.9837, 172.5877, 177.5081, 170.5384, 169.4296, 178.5278, 177.4982,\n",
      "        177.5872, 176.4191, 175.7954, 180.1910, 177.9041, 173.1718, 170.2810,\n",
      "        170.4493, 170.4691, 173.3302, 173.7757, 170.4691, 171.3403, 160.0047,\n",
      "        158.1831, 158.0941, 157.6287, 168.6277, 173.0332, 172.8649, 174.0826,\n",
      "        171.1720])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0800, 0.0798, 0.0799]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 224.1800, 224.3100, 223.9600])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0795, 0.0796, 0.0803]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 451.0600, 444.3000, 447.5900])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0805, 0.0805, 0.0807]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 546.1700, 549.1000, 530.3000])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0842, 0.0843, 0.0843]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 114.3175, 111.5254, 107.8938])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0846, 0.0846, 0.0844]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 121.4900, 120.9000, 120.9000])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0873, 0.0876, 0.0877]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 22.3107, 22.0228, 21.4663])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0876, 0.0876, 0.0876]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 19.9300, 19.8300, 20.8000])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0807, 0.0805, 0.0802]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 151.3400, 155.0200, 155.4100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0801, 0.0802, 0.0803]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 150.6000, 155.7800, 154.4300])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0811, 0.0812, 0.0813]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 170.8000, 171.2760, 170.2900])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0813, 0.0813, 0.0811]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 177.5800, 174.1200, 173.5100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0805, 0.0803, 0.0802]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 168.7437, 168.1883, 168.5098])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0802, 0.0804, 0.0805]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 169.9626, 173.4604, 177.7334])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0839, 0.0838, 0.0838]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 97.4832, 95.8793, 95.8793])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0858, 0.0857, 0.0857]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 73.3004, 73.7103, 73.2073])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0853, 0.0852, 0.0852]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 76.8407, 75.6828, 75.3983])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0799, 0.0799, 0.0799]) \n",
      "  vs \n",
      " labels: tensor([  53.7432,   53.4660,   53.2058,  ..., 1903.3300, 1898.3400,\n",
      "        1902.8000])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0799, 0.0798, 0.0799]) \n",
      "  vs \n",
      " labels: tensor([  53.7432,   53.4660,   53.2058,  ..., 2620.4800, 2638.3501,\n",
      "        2608.2300])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0808, 0.0809, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 360.6677, 360.2006, 358.2270])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0813, 0.0815, 0.0814]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 485.3288, 485.1196, 487.3508])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0803, 0.0805, 0.0805]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 238.4300, 237.5500, 240.1100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0811, 0.0810, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 250.6925, 248.8050, 248.5254])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0846, 0.0849, 0.0852]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 66.5600, 71.6400, 72.6600])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0883, 0.0882, 0.0883]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 42.6012, 42.4219, 42.2516])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0877, 0.0877, 0.0878]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 47.5595, 47.0221, 48.5575])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0882, 0.0882, 0.0883]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 45.7040, 46.5123, 46.2429])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0883, 0.0884, 0.0886]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 39.4500, 38.8800, 38.1024])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0800, 0.0805, 0.0806]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 191.7600, 197.0900, 197.5100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0827, 0.0827, 0.0826]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 90.1500, 90.4700, 88.6200])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0798, 0.0797, 0.0800]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 275.1002, 278.8859, 279.8228])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0803, 0.0803, 0.0805]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 224.6748, 226.5045, 229.2888])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0907, 0.0906, 0.0906]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 29.7327, 30.7049, 32.0732])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0885, 0.0886, 0.0886]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 43.0744, 42.1874, 42.6163])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0818, 0.0818, 0.0819]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 122.0895, 122.4445, 125.2575])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0815, 0.0815, 0.0815]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 135.1549, 136.2536, 137.0427])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0809, 0.0809, 0.0810]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 306.3843, 304.8117, 311.0271])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0807, 0.0808, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 300.5463, 296.6729, 295.9669])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0812, 0.0810, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 177.8253, 176.4973, 176.9666])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0897, 0.0898, 0.0897]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 29.3013, 29.1573, 29.4166])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0896, 0.0895, 0.0894]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 34.0900, 33.3400, 33.7100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0815, 0.0814, 0.0815]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 147.5511, 148.9926, 147.6848])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0804, 0.0804, 0.0805]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 226.9905, 227.6370, 223.0819])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0820, 0.0820, 0.0819]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 149.1186, 148.8118, 148.1888])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0815, 0.0815, 0.0815]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 155.7392, 155.6797, 155.5308])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0831, 0.0832, 0.0831]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 96.0303, 93.6529, 91.8861])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0817, 0.0818, 0.0816]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 128.6499, 125.3289, 124.9939])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0861, 0.0862, 0.0863]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 14.5296, 14.5016, 13.7084])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0872, 0.0872, 0.0872]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 16.7498, 16.9140, 17.1264])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0870, 0.0870, 0.0870]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 41.6578, 40.7542, 41.0097])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0860, 0.0858, 0.0857]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 58.2174, 59.4203, 58.5304])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0884, 0.0883, 0.0883]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 44.9177, 44.6784, 45.4728])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0876, 0.0876, 0.0875]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 49.7199, 50.0526, 49.6807])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0862, 0.0862, 0.0862]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 57.0749, 56.5191, 52.7557])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0883, 0.0883, 0.0883]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 36.2030, 35.8703, 35.4103])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0822, 0.0822, 0.0822]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 125.3932, 126.2414, 126.4246])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0809, 0.0809, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 190.2500, 189.8000, 192.9800])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0818, 0.0816, 0.0817]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 339.0583, 346.1629, 345.8359])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0815, 0.0816, 0.0816]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 483.3400, 487.0900, 487.0900])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0814, 0.0813, 0.0814]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 162.2051, 160.5190, 153.5580])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0805, 0.0805, 0.0805]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 239.8600, 237.5200, 242.5200])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0803, 0.0801, 0.0802]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 243.7235, 242.1830, 241.7523])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0802, 0.0801, 0.0801]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 275.0940, 274.9648, 273.8222])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0815, 0.0815, 0.0815]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 168.8283, 166.2671, 167.8804])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0848, 0.0848, 0.0848]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 75.2417, 76.7081, 75.7698])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0811, 0.0812, 0.0811]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 275.4528, 276.9527, 275.5018])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0809, 0.0810, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 377.5296, 373.1457, 367.7953])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0805, 0.0808, 0.0809]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 637.9700, 639.0000, 625.1400])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0810, 0.0814, 0.0810]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 630.0800, 617.1400, 636.1800])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0807, 0.0807, 0.0807]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 166.6773, 164.8706, 161.7257])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0827, 0.0827, 0.0828]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 100.0204, 101.9857, 103.1072])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0895, 0.0893, 0.0892]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 14.2820, 14.7100, 14.7180])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0867, 0.0869, 0.0870]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 10.8000, 10.6800, 10.9800])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0825, 0.0826, 0.0821]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 161.0794, 158.8359, 153.3384])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0797, 0.0806, 0.0807]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 468.3114, 487.7998, 492.5994])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0831, 0.0832, 0.0832]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 121.7651, 121.3892, 120.9193])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0826, 0.0826, 0.0825]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 149.0807, 149.0509, 148.4447])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0865, 0.0865, 0.0865]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 50.0927, 49.8946, 48.9041])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0874, 0.0873, 0.0874]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 18.6400, 18.5600, 19.0300])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0884, 0.0882, 0.0882]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 32.5200, 32.0200, 32.3400])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0898, 0.0897, 0.0896]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 41.9260, 41.9260, 40.6840])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0787, 0.0787, 0.0787]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ...,  4.6333,  4.6617,  4.6712])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0848, 0.0847, 0.0847]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 79.2045, 79.3060, 79.6750])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0839, 0.0840, 0.0841]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 90.2456, 88.7469, 87.1516])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0848, 0.0850, 0.0851]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 75.3700, 75.2900, 75.7500])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0821, 0.0821, 0.0820]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 149.9700, 149.9200, 154.6100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0873, 0.0873, 0.0873]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 62.1500, 64.4000, 64.0200])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0812, 0.0812, 0.0812]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 246.6393, 247.9765, 246.3528])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0819, 0.0817, 0.0817]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 163.8819, 162.2035, 163.3456])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0818, 0.0818, 0.0818]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 173.8800, 172.8600, 174.2300])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0815, 0.0819, 0.0818]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 218.8600, 223.0500, 226.3100])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0805, 0.0807, 0.0808]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 245.3400, 242.6500, 239.7600])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0894, 0.0892, 0.0892]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ...,  9.2100,  9.2500,  9.0500])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0870, 0.0870, 0.0869]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ...,  4.4700,  4.7000,  4.9600])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0871, 0.0871, 0.0871]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 49.2170, 49.3750, 49.2078])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0871, 0.0871, 0.0871]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 49.5716, 49.4328, 48.9369])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0797, 0.0797, 0.0797]) \n",
      "  vs \n",
      " labels: tensor([ 53.7432,  53.4660,  53.2058,  ..., 227.5775, 226.9400, 231.8013])\n",
      "f:predictions:\n",
      " tensor([0.0845, 0.0845, 0.0846,  ..., 0.0841, 0.0841, 0.0842]) \n",
      "  vs \n",
      " labels: tensor([53.7432, 53.4660, 53.2058,  ..., 68.3300, 69.9400, 70.1100])\n",
      "Average Loss: 2711.1221\n"
     ]
    }
   ],
   "source": [
    "evaluation_function(model=Stock_predictor,test_loader=test_loader,loss_function=loss_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c71237-b94a-448d-b7dd-b74aea979d26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
